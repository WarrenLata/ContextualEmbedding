{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "366729fd",
   "metadata": {},
   "source": [
    "# Librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94e04963",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wawa/anaconda3/envs/vectorization/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8466f583",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4870acef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it's not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn't have much of a plot.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load IMDb dataset\n",
    "raw = load_dataset(\"imdb\")\n",
    "\n",
    "# Combine train + test into a single dataset (50k rows)\n",
    "all_data = concatenate_datasets([raw[\"train\"], raw[\"test\"]])\n",
    "texts = all_data['text']\n",
    "print(len(all_data))  # 50000\n",
    "print(all_data[0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e195219",
   "metadata": {},
   "source": [
    "## stritified k-folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00cc9170",
   "metadata": {},
   "outputs": [],
   "source": [
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=10000, random_state=42)\n",
    "y      = np.array(all_data['label'])\n",
    "_, idx_1k = next(sss.split(texts, y))\n",
    "texts_1k = np.array(texts)[idx_1k]\n",
    "y_1k     = y[idx_1k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "857d21e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "splits = list(skf.split(np.zeros(len(y_1k)), y_1k))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60756d65",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "43dd6677",
   "metadata": {},
   "source": [
    "## Transformers model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec40ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "device ='cpu'\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"  \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "encoder   = AutoModel.from_pretrained(model_name).to(device)\n",
    "\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "class TransformersEmbedder(nn.Module):\n",
    "    def __init__(self, tokenizer, encoder, device,batch_size: int = 64,max_length=512,stride:int=128):\n",
    "        super().__init__()\n",
    "        self.tokenizer =  tokenizer\n",
    "        self.encoder  = encoder.eval()\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "        self.max_length = max_length\n",
    "        self.stride = stride\n",
    "\n",
    "\n",
    "    def forward(self, texts):\n",
    "        \n",
    "        embs = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i in range(0, len(texts), self.batch_size):\n",
    "                batch = texts[i:i+self.batch_size]\n",
    "                encoded = self.tokenizer(\n",
    "                    batch,\n",
    "                    padding=True,\n",
    "                    truncation=True,\n",
    "                    return_tensors=\"pt\",\n",
    "                    return_overflowing_tokens=True,\n",
    "                    stride = self.stride,\n",
    "                    max_length = self.max_length\n",
    "                ).to(self.device)\n",
    "            \n",
    "                output = self.encoder(**encoded)\n",
    "                embeddings = mean_pooling(output, encoded['attention_mask']) # CLS token\n",
    "                embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "                embs.append(embeddings)\n",
    "        return np.vstack(embs)  # shape: (B, hidden_size)\n",
    "    \n",
    "\n",
    "model = TransformersEmbedder(tokenizer, encoder, device,64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2504e1f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['This is an atrocious movie. Two demented young women seduce and torture a middle aged man. There\\'s not much to give away in regards to a plot or a \"spoiler\". I would only comment that the ending is nearly the most preposterous part of the flick. Much of the film involves Locke and Camp cackling obnoxiously, all the while grinning psychotically at the camera. Add to this a soundtrack that repeats again and again, including a vaudevillian song about \"dear old dad\" that suggests an incestuous quality the viewer never really sees. The music is annoying at first, then ends up subjecting the viewer to a torture worse than that depicted on the screen. The theme here is of youth run amok, understandable as a reaction to the \\'60s, but done with little imagination or style. Avoid it!',\n",
       "       'well,there isnt much to say about this movie. its simply trash. very poor acting, poor script, and lame story.... well, the actress,(i odnt even know her name) who played mainrole,(not the blond one,but latina one) was acting fine,but the blond one who played the friend of main charactor,,,her acting level is just like highschool play,so as most of other actors in the movie. Also,zombies,,,,very bad acting as well. and,,the story itself has really no point at all. well, if you are really bored and really got nothing to do,but wanna kill time somehow, maybe you may wanna watch this movie,but eventho,there are still millions of better B movies than this crap. its total waste of money and time.',\n",
       "       'Surprisingly good made for T.V. Thriller. I wasn\\'t expecting too much from this one but I\\'m glad to say that this is one of the best of it\\'s kind. It\\'s fast paced and features solid acting and interesting events. <br /><br />The story gets you hooked on since the beginning and with many hits, you can\\'t help but find the movie very interesting.<br /><br />The background story of Thiessen\\'s character is hard but it turns into a nightmare when her husband is something much worse than her childhood friend\\'s step father.<br /><br />The \"stranger\" concept and his actions are disturbing if you consider that it is a common disease in society. A serial rapist does not respect society or even his own family and this movie displays the sickness and crime in a perfect way. <br /><br />My beef with the movie was the non sense situation when Thiessen\\'s character returns home with her deranged husband after he\\'s released on bail. By that moment she knew what he did to his young relative (played by the cute Allyson Hannigan). But that\\'s just the typical Hollywood scene that provokes more trouble. <br /><br />The climax scene shows Thiessen following her husband when he\\'s about to commit another rape, then she calls the cops who come in time just to capture him. The ending tells us that the husband was sentenced for 99 years in prison in real life. <br /><br />The acting is also pretty good, and Tiffany is great and gorgeous as always. The direction is also very good.',\n",
       "       ...,\n",
       "       'Harsh, yes, but I call \\'em like I see \\'em.<br /><br />I saw this in the late 80\\'s, and it was truly one of the most awful, boring films I\\'ve ever forced myself to watch.<br /><br />Yes, the cinematography is lovely. The Czech settings are truly stunning. The political backdrop is enticing, but unlike similar \"historically set\" stories (e.g. _Dr. Zhivago_ (qv)), this one failed to make the politics relevant to the story, or even interesting.<br /><br />Sure, Olin and Binoche are beautiful. But this film manages to make even \"erotic\" scenes plodding and slow. I\\'m all for romance, but this movie was so boring, I started hoping the Russians would shoot them all and put an end to my misery.<br /><br />I\\'m sure if I\\'d read the book, the story would have made a bit more sense. However, life\\'s too short to expend any more time on this one.',\n",
       "       'Talk about your classics! Ernie Fossilus (the Foss from here on out) came up with a cute and creative trailer totally spoofing Star Wars. This gem is so jammed packed with tributes and gags I laugh every time! Not only that, when Star Wars did a re-issue with new special effects, Hardware Wars did the same! Talk about a spoof that just won\\'t die! There\\'s a reason George Lucas calls this his favorite parody. He was so impressed, he even hired the Foss to work on \"Return of the Jedi\" (Don\\'t believe me, check his entry in IMDb!)<br /><br />This has to be the first, and in my opinion, the best parody ever done. I think the Special Edition was a bit overdone, but on reflection, I think it\\'s PERFECT for the modern day re-release of Star Wars, and goes to prove that sometimes, it\\'s wrong to mess with perfection.<br /><br />Yes, it\\'s only 10 minutes, but it\\'s well worth your time.<br /><br />You\\'ll laugh, you\\'ll cry, you\\'ll kiss $3 goodbye! Well, maybe 15 for the DVD, but you\\'ll be real happy you did.',\n",
       "       \"I do admit that my review is from a 2006 point of view, nearly 30 years after the making of this movie and at the time of its conception, it may have been a brilliant horror/thriller movie.<br /><br />Beginning on Halloween night 1968, 6 year old Michael commits the brutal murder of his 18 year old sister. Michael is committed to a mental institution and 15 years later escapes and returns to his home town to murder again.<br /><br />From this point it is clear that the movie will follow a basic and rudimentary path that is highly predictable. The beginning of every scene is easily predictable in the way it will end whilst the music for each scene containing Michael (the murderer) is exactly the same throughout the movie, thus alerting the viewer to the likely events to follow.<br /><br />For the horror/thriller enthusiast, there is a severe lack of blood/gore compared to modern day films although I am not akin to the amount that is displayed in this day and age. A happier medium could have been found.<br /><br />From a half hour into the movie, not one scene is unexpected. The acting for a horror/thriller film is fairly typical of the era and thus lacks any punch for the modern day enthusiast.<br /><br />A positive for the film is its lingering camera shots and dark lighting which creates a frightening atmosphere. A second positive would be the character of Michael's doctor, who provides clues to the probability of where the story may lead.<br /><br />However, it is clear that the star, in Jamie-Lee Curtis, is in the infancy stages of her acting career and thus fails to provide a truly frightened central victim.<br /><br />It is hard for me to rate this film as it was in its day, but from other horror/thriller films of indeed the 80s and to a lesser extent, the 90s, it falls far short of a truly great horror/thriller film.<br /><br />I suggest you move on and find a classic from the 80s. Cheers!\"],\n",
       "      shape=(10000,), dtype='<U13704')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_1k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d12d521",
   "metadata": {},
   "outputs": [],
   "source": [
    "All_mini_vectors = model(list(texts_1k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "788a3623",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1196"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts_1k[1611].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "ab52b132",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([288, 144, 123, ..., 117,  49, 171], shape=(10000,))"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "85ada65d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1611,\n",
       " 2048,\n",
       " 2265,\n",
       " 2803,\n",
       " 3110,\n",
       " 5124,\n",
       " 5461,\n",
       " 5493,\n",
       " 5957,\n",
       " 6322,\n",
       " 6522,\n",
       " 6804,\n",
       " 7049,\n",
       " 8046,\n",
       " 8292,\n",
       " 8387,\n",
       " 8597,\n",
       " 9303,\n",
       " 9799]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in range(0,len(texts_1k)) if len(texts_1k[i].split())>1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "39d2a5e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['This is an atrocious movie. Two demented young women seduce and torture a middle aged man. There\\'s not much to give away in regards to a plot or a \"spoiler\". I would only comment that the ending is nearly the most preposterous part of the flick. Much of the film involves Locke and Camp cackling obnoxiously, all the while grinning psychotically at the camera. Add to this a soundtrack that repeats again and again, including a vaudevillian song about \"dear old dad\" that suggests an incestuous quality the viewer never really sees. The music is annoying at first, then ends up subjecting the viewer to a torture worse than that depicted on the screen. The theme here is of youth run amok, understandable as a reaction to the \\'60s, but done with little imagination or style. Avoid it!',\n",
       "       'This contains some spoiler information, but the movie is not worth watching anyway...<br /><br />Why Ellen Barkin and Peta Wilson agreed to be a part of this debacle by writer-director Damian Harris is beyond me. The story is full of unrealistic police investigating techniques, which includes going to a party with a suspect and inviting that suspect over to your house and getting intimate with them. The tale also features a male psychiatrist who seems to have nothing but female clients - and he sleeps with them all.<br /><br />Even more over-the-top is the notion that the female victims to a horrendous S/M crime belong to a large, yet secret, group of S/M lesbian executives.<br /><br />Full of bad editing and continuity errors, the film is sterile in all of its ritzy locations - including Barkin\\'s detective salary apartment. And the lame dialogue is fit to put one to sleep. <br /><br />Damian is a bad writer and a bad director. He tried and failed miserably to create a noiresque atmosphere at times. Furthermore, he couldn\\'t get Ellen Barkin to give 110%. I firmly believe she realized the film was pathetic and gave up trying. <br /><br />Standouts were Peta Wilson, who wasted time studying with a Dominatrix for the part. The highly under-rated character actor, Marshall Bell gave his absolute best as always.<br /><br />And since Damian couldn\\'t deliver on any level, it was hard to feel emotion for any of the characters. <br /><br />Most importantly, he doesn\\'t know much about the real world of BDSM and chose to create the typical \"Hollywood Reality\" of gorgeous men and women who are perverted and dangerous.<br /><br />Save your time, money and braincells and pass this one by.',\n",
       "       'Dreamy young Ashton Kutcher (as Tom Stansfield) wants a date with sexy blonde Tara Reid (as Lisa Taylor). Ms. Reid thinks Mr. Kutcher is gay. Kutcher works for Reid\\'s father, an anal retentive Terence Stamp (as Jack Taylor). Kutcher agrees to \"housesit\" for the boss, believing it will get him closer to Reid. Mr. Stamp has a pet owl named \"O.J.\", who becomes a toilet cokehead. <br /><br />This is a film to get your restricted to \"G-rated\" pre-teens ready for raunchier \"R-rated\" fare. It will help if they haven\\'t seen the plot before, and especially like moronic potty humor. Remember, people get paid to act like this.<br /><br />** My Boss\\'s Daughter (2003) David Zucker ~ Ashton Kutcher, Tara Reid, Terence Stamp',\n",
       "       ...,\n",
       "       \"Saw this at Sundance one of the first years I attended the festival - blew me away...<br /><br />It went by much too fast, and I've never been able to find it again. I figured it would show up on a compilation somewhere, but still no joy. My memory's foggy on the details - I just remember being surprised over and over again, and on the edge of my seat.<br /><br />Anyone reading this have any leads? Online, DVD, beat-up VHS copy - I'd try to make it worth your while...<br /><br />Over the years, I've probably seen over 200 short films (used to live near Aspen where they have an incredible Shorts Fest in April). This definitely ranks among my favorites, and I hope to see it again!<br /><br />Many thanks to anyone with leads on how to see it :)\",\n",
       "       \"William Haines sparks this tale of a brash cadet who thinks West Point will really be something now that he has arrived. Terrific goony comic performance by Haines was his trademark--one that made him a top box office star from 1928-1932 and one of MGM's biggest stars. Joan Crawford and William Bakewell are fine too. And although this storyline may seem trite now, this was a huge hit, putting Haines and Crawford in a college football (a national craze during the 20s) story. After Haines blows off his big mouth one time too often and nearly gets shunned by fellow cadets, he turns in a wonderful performance as he swallows his pride and gets into the big game against Navy. Even with a broken arm, he wins the game for Army and regains his place at West Point. It's easy to see from this film and Show People (with the always underrated Marion Davies) why Billy Haines was a huge star of the time. He needs and deserves to be remembered!\",\n",
       "       \"The Mummy's Curse is the last in the series of the Kharis mummy films, and it seems that creativity had run somewhat dry by the time they made this one. Kharis and his mate Ananka both end up in the bayous of Louisiana, and on his resurrection, he searches out his beloved princess. How they end up in Louisiana isn't made entirely clear, but with various people trying to find them, the viewer can be assured of some mummy murders.<br /><br />The Mummy's Curse was watchable, but it really wasn't anything special. I had the feeling throughout the movie that I'd seen this before. Quite frankly, apart from the original The Mummy with Boris Karloff, the Mummy movies are not my favorites among the old Universal horrors. They're not bad, but they do get a bit repetitive.<br /><br />There are some great scenes in the movie (the scene with Ananka coming to life in the swamp for instance) but overall this didn't do much for me.<br /><br />It's worth seeing if you want to be completest and see all of the mummy movies, but otherwise you might want to pass on this one.\"],\n",
       "      shape=(3341,), dtype='<U13704')"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = (bins == 2)\n",
    "idx\n",
    "texts_1k[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "3983e4cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.3034,  0.0221,  0.1216,  ...,  0.3184,  0.0161, -0.1632],\n",
       "         [ 0.0139,  0.0014, -0.0302,  ...,  0.1748,  0.0678,  0.1497],\n",
       "         [-0.0948, -0.2980,  0.0994,  ...,  0.7606, -0.0301, -0.2858],\n",
       "         ...,\n",
       "         [ 0.0867,  0.1850, -0.0683,  ...,  0.1929,  0.2885,  0.2349],\n",
       "         [ 0.0859, -0.0008,  0.0658,  ...,  0.5383, -0.3277,  0.1860],\n",
       "         [ 0.0800, -0.0134, -0.1270,  ...,  0.2229,  0.2033, -0.0546]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-3.9288e-02,  4.9246e-02,  3.6811e-02,  5.4538e-02, -8.7551e-02,\n",
       "         -3.6292e-02,  4.6885e-02,  8.0314e-03, -4.6500e-02,  3.4658e-02,\n",
       "          1.2285e-02, -2.6979e-02,  3.8243e-02, -2.3428e-02, -6.3229e-02,\n",
       "          1.3954e-02,  1.0081e-01, -1.8101e-02, -5.9666e-03,  5.9562e-02,\n",
       "         -1.4654e-02,  1.2304e-02,  1.6934e-02, -7.6535e-02, -5.9665e-04,\n",
       "          9.3974e-02,  1.9916e-02,  6.1776e-02,  2.9831e-02,  9.1364e-02,\n",
       "          5.3837e-02,  1.4959e-02, -7.8535e-02,  3.2495e-02, -3.5960e-02,\n",
       "         -7.7912e-02, -2.0958e-02,  1.0483e-01, -5.7155e-02,  4.2787e-04,\n",
       "          5.5340e-02,  1.0756e-02, -4.4274e-02, -6.1725e-03, -9.5563e-03,\n",
       "         -8.4004e-03, -1.1778e-01, -5.2717e-02,  2.5263e-02,  5.1330e-02,\n",
       "         -9.3422e-02,  7.1472e-02,  4.4061e-02,  1.6573e-02, -6.3343e-02,\n",
       "          1.6688e-02,  5.3931e-02,  1.5547e-02,  7.0728e-02,  4.7442e-02,\n",
       "          4.2392e-02, -3.5225e-02, -4.8147e-02, -3.9347e-03, -3.6799e-02,\n",
       "         -1.0031e-02, -7.7142e-03, -7.1194e-02, -7.9465e-02, -1.8201e-02,\n",
       "         -5.9028e-02,  5.8574e-03, -1.8587e-03,  1.9846e-02, -7.4564e-03,\n",
       "          3.4444e-02, -6.9168e-02,  4.7451e-03, -7.7043e-02,  7.9384e-02,\n",
       "          2.3429e-02,  4.0294e-02,  3.1018e-02, -1.0083e-01, -8.6791e-02,\n",
       "         -8.5183e-02, -6.2904e-02,  9.1905e-02, -1.9436e-02,  6.9437e-02,\n",
       "          8.6390e-02, -3.3657e-02, -1.2693e-01,  8.1809e-03,  1.2773e-01,\n",
       "         -3.2713e-02, -2.0740e-02, -5.3275e-02, -1.3401e-01, -1.0634e-02,\n",
       "          8.5665e-02, -3.6297e-02, -2.0608e-02,  1.7395e-02,  2.7155e-02,\n",
       "         -5.1831e-02,  1.4337e-02,  8.2276e-02, -3.3855e-02,  1.7053e-02,\n",
       "          9.8530e-02,  7.0834e-02,  7.5373e-02,  2.1345e-02, -4.0607e-02,\n",
       "          2.3963e-02, -3.3264e-02, -2.1931e-02, -1.0117e-01,  5.4527e-02,\n",
       "          1.3786e-01, -6.7924e-02,  2.8484e-02,  4.2803e-02,  3.9898e-02,\n",
       "          2.0220e-02,  3.3107e-02, -1.0088e-01, -9.8882e-02, -3.0960e-02,\n",
       "          1.6182e-01, -5.3266e-02,  3.6195e-02, -1.3621e-01, -6.9005e-02,\n",
       "          5.8311e-02, -6.6971e-02, -2.1539e-02, -6.7271e-02,  1.1892e-01,\n",
       "          6.3763e-02, -1.7113e-01,  5.0621e-02, -4.5347e-02,  6.9568e-02,\n",
       "          5.9236e-02, -7.8987e-02, -1.1078e-01,  9.3390e-03, -6.0688e-02,\n",
       "          9.2085e-02, -4.3844e-02, -3.4967e-02,  2.6120e-02,  1.8023e-02,\n",
       "          1.1768e-02,  8.7769e-03, -1.7136e-07,  4.4306e-03, -1.0028e-01,\n",
       "         -6.0733e-02, -1.0740e-01,  8.1407e-02,  7.9733e-02,  6.7039e-04,\n",
       "          3.2164e-02,  2.5861e-02, -1.4555e-02,  8.7538e-04, -8.0682e-02,\n",
       "         -5.7851e-02, -1.3943e-02, -4.8460e-02, -8.0954e-02,  4.1349e-02,\n",
       "          2.5994e-02,  1.5872e-02, -5.1029e-02,  1.0285e-02, -5.8401e-02,\n",
       "          9.8354e-03, -8.4778e-02, -2.4708e-02,  2.1277e-02,  7.1076e-02,\n",
       "          6.9037e-02,  1.1034e-02,  1.7697e-02,  8.8318e-02, -7.9722e-02,\n",
       "          2.1404e-02,  2.3778e-02,  2.5182e-02,  6.8846e-02,  1.5589e-02,\n",
       "         -2.4752e-02, -3.8274e-02,  4.0381e-02,  2.4813e-02,  2.2298e-02,\n",
       "          3.5099e-02,  9.0757e-02,  3.3824e-02,  4.1818e-03,  1.0649e-02,\n",
       "         -4.8541e-02, -6.5684e-02, -1.5188e-01, -1.9312e-02,  8.1450e-03,\n",
       "          1.0797e-01,  8.4732e-02, -3.7752e-02, -2.8204e-02, -7.7585e-02,\n",
       "         -9.1184e-02,  6.0913e-02,  3.4268e-02,  5.2971e-02, -1.8249e-02,\n",
       "          3.3919e-02,  4.4513e-02,  2.9830e-02, -9.1895e-02, -1.5993e-01,\n",
       "         -3.1808e-02, -1.7456e-02, -1.0979e-01, -3.8463e-03,  1.5772e-02,\n",
       "         -5.0024e-02,  9.2618e-02,  2.2390e-02,  1.1083e-01,  9.4975e-03,\n",
       "          3.9507e-02, -6.3702e-04, -6.6902e-02,  6.1847e-02,  7.2427e-02,\n",
       "         -1.5619e-01,  7.2285e-02, -2.6062e-02, -2.3774e-02, -5.3032e-02,\n",
       "          6.1422e-02,  1.0021e-01,  2.5876e-02, -7.2251e-03, -9.0980e-02,\n",
       "          8.9638e-03, -1.2265e-02,  1.1320e-02,  2.7808e-02, -5.5409e-02,\n",
       "          5.5570e-02,  8.7169e-02,  5.6508e-03,  2.3906e-02,  2.4652e-02,\n",
       "          2.6749e-02, -2.2293e-03, -2.7027e-02, -3.3693e-02, -8.4781e-02,\n",
       "          2.4261e-02,  8.6155e-02, -4.8591e-02, -4.1781e-03, -1.0286e-01,\n",
       "          4.2471e-02,  6.0032e-03, -6.6661e-02, -3.5235e-02,  7.6572e-02,\n",
       "          8.7895e-03,  4.7981e-02, -9.9669e-02,  8.5586e-02, -2.2902e-02,\n",
       "         -2.1821e-02,  7.7193e-02,  7.8886e-02, -2.9400e-02,  6.7892e-02,\n",
       "         -3.8921e-02,  4.1224e-02, -3.5276e-02,  4.6029e-02,  1.2437e-01,\n",
       "         -2.2968e-02, -8.3755e-03, -2.8387e-02,  1.7179e-02, -3.9077e-02,\n",
       "          6.6730e-02,  7.5254e-02,  1.0142e-01, -2.4546e-02,  4.7730e-02,\n",
       "         -7.7881e-03, -1.9138e-02,  5.3961e-02, -6.1914e-02, -6.3136e-02,\n",
       "          1.3900e-02, -4.9335e-02, -1.3638e-02, -3.9959e-02, -1.5257e-02,\n",
       "          1.0959e-01,  1.9817e-02,  2.9849e-04,  4.8909e-02, -2.3418e-02,\n",
       "          9.0017e-03,  5.2616e-02, -8.8414e-02, -1.0328e-02, -4.1129e-02,\n",
       "          4.1919e-02, -1.1088e-01,  1.4614e-03,  4.3812e-02,  4.5769e-02,\n",
       "         -2.1153e-02, -3.2954e-03,  3.7862e-02, -3.6558e-02,  9.3943e-03,\n",
       "          9.2893e-02, -6.3271e-03,  5.7511e-02,  9.7323e-02,  5.8023e-02,\n",
       "         -1.1405e-01,  5.1513e-02,  6.8802e-02,  6.4103e-02,  9.6646e-03,\n",
       "          3.6647e-02, -6.2027e-02,  5.9836e-02, -5.7129e-02,  6.7215e-02,\n",
       "         -1.1129e-01,  5.8965e-02,  4.3591e-02,  1.3474e-02, -3.2926e-02,\n",
       "         -6.0149e-02, -6.7591e-03,  8.5814e-02, -9.8462e-03, -5.4692e-02,\n",
       "         -9.2863e-02, -4.2358e-02,  2.5231e-04,  4.0398e-02, -1.3405e-03,\n",
       "          2.4400e-02, -3.3935e-02,  1.0590e-03, -8.2071e-02,  2.3761e-02,\n",
       "         -2.8557e-02,  7.3383e-04, -5.4955e-02,  4.9863e-02, -3.0982e-02,\n",
       "         -8.8824e-02,  6.8481e-02,  1.3088e-01,  3.0891e-02, -6.2133e-02,\n",
       "         -4.5612e-02, -1.0350e-01,  3.5948e-02, -8.9030e-02, -5.5466e-02,\n",
       "          1.9186e-02,  3.6834e-02, -5.6200e-02, -3.5800e-02]],\n",
       "       grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder(**tokenizer(\n",
    "                   texts_1k[1611] ,\n",
    "                    padding=True,\n",
    "                    truncation=True,\n",
    "                    return_tensors=\"pt\",\n",
    "                    # return_overflowing_tokens=True,\n",
    "                    # stride=128,\n",
    "                    max_length = 512\n",
    "                ))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "adf71258",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,  1996, 18458,  ...,  2009,  2157,   102],\n",
       "        [  101,  1006,  2585,  ...,  1997,  1996,   102],\n",
       "        [  101,  5859, 14855,  ..., 25492,  1000,   102],\n",
       "        [  101,  2219,  3399,  ...,     0,     0,     0]])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\n",
    "                   texts_1k[1611] ,\n",
    "                    padding=True,\n",
    "                    truncation=True,\n",
    "                    return_tensors=\"pt\",\n",
    "                    return_overflowing_tokens=True,\n",
    "                    stride=128,\n",
    "                    max_length = 512\n",
    "                )['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "0e6b4c01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,  1996, 18458,  1997,  2023,  3185,  2038,  2042, 16356,  2989,\n",
       "          2026,  9647,  2005,  3243,  2070,  2051,  2085,  1012,  2057,  1005,\n",
       "          2310,  2035,  2657,  2030,  3191,  2055,  2009,  1999,  2070,  2785,\n",
       "          1997,  9530,  1011,  3793,  1012,  2054,  2052,  2017,  2079,  2065,\n",
       "          2017,  2020,  2035,  2894,  1999,  1996,  2088,  1029,  2054,  2052,\n",
       "          2017,  2079,  2065,  1996,  2972,  2088,  3402,  5419,  1999,  2392,\n",
       "          1997,  2115,  2159,  1029,  1999,  2755,  1010,  1996,  2197,  2112,\n",
       "          2003,  2941,  2054,  6433,  2000,  4913,  1998,  4080,  1010,  2048,\n",
       "          2282,  1011, 14711,  2542,  1999,  1037,  2448,  1011,  2091,  2160,\n",
       "          1999,  1996,  2690,  1997,  1037, 10846,  2291,  1012,  4080,  2003,\n",
       "          1037,  6091, 12006,  2000,  2360,  1996,  2560,  1998,  4913,  2003,\n",
       "          2641,  2108,  2028,  1997,  1996,  5221, 23160,  1997,  2554,  1012,\n",
       "          2008,  2894,  2003,  1996,  2364,  3114,  2000,  2339,  2122,  2048,\n",
       "          4364,  2131,  2061,  2092,  2247,  1010,  2138,  2027,  3432,  2069,\n",
       "          2031,  2169,  2060,  2000,  2735,  2000,  2043, 16334,  2003,  2734,\n",
       "          1012,  2074,  2127,  1012,  1012,  1012,  1026,  7987,  1013,  1028,\n",
       "          1026,  7987,  1013,  1028,  3442,  2013,  1996,  2927,  1997,  1996,\n",
       "          2143,  7167,  1998,  7167,  1997,  3471,  4148,  2000,  2068,  1012,\n",
       "          2119,  1997,  2068,  2131,  2920,  2007,  4126,  1010,  4080, 17567,\n",
       "          2013, 27890,  1998,  3432,  2987,  1005,  1056,  8108,  2183,  2041,\n",
       "          1997,  1996,  2160,  1012,  4913,  2003,  7736,  2012,  2010,  3105,\n",
       "          1998,  2010,  8628,  2123,  1005,  1056,  7438,  2032,  2200,  2092,\n",
       "          1998,  2007,  1996,  4847,  2002, 17210,  1012,  1996,  3815,  1997,\n",
       "          3471,  2027,  2227,  7906,  4852,  2127,  2008,  2028,  2154,  2073,\n",
       "          2027,  2089,  2031,  2000,  2227,  1996, 13418,  1998,  3066,  2007,\n",
       "          2009,  1012,  2023,  2003,  2074,  2205,  2172,  2005,  2068,  1998,\n",
       "          2027,  4299,  2008,  2673,  2052,  2074,  2175,  2185,  1012,  1012,\n",
       "          1012,  1998,  1997,  2607,  2008,  2003,  3599,  2054,  6433,  1012,\n",
       "          1026,  7987,  1013,  1028,  1026,  7987,  1013,  1028,  1996,  2717,\n",
       "          1997,  1996,  2466,  3182,  4913,  1998,  4080,  1999,  2023,  2088,\n",
       "          1997,  2498,  2791,  1012,  2012,  2034,  2027,  2024,  4527,  1998,\n",
       "          2031,  3471,  4824,  1998,  7149,  2007,  1996,  2838,  1997,  2023,\n",
       "          4689,  4044,  1010,  2021,  2101,  2006,  2027,  2424,  2041,  2008,\n",
       "          2027,  2064,  2079,  2074,  2055,  2673,  2027,  2215,  2138,  2009,\n",
       "          3849,  2004,  2065,  2027,  2024,  1996,  2069,  3924,  2145,  2187,\n",
       "          1012,  1026,  7987,  1013,  1028,  1026,  7987,  1013,  1028,  2498,\n",
       "          2838,  2019, 11757,  2235,  3459,  1011,  1999,  2755,  1010,  4661,\n",
       "          1996,  2034,  3232,  1997,  7171,  2013,  1996,  2143,  1010,  2057,\n",
       "          2069,  2156,  4913,  1006,  2585,  2002, 13668,  6582,  1007,  1998,\n",
       "          4080,  1006,  4080,  4679,  1007,  1999,  1996,  2972,  2143,  1012,\n",
       "          2009,  2003,  3154,  2008,  1999,  2344,  2000,  4139,  2023,  2125,\n",
       "          1010,  1996,  3459,  2038,  2000,  2022,  2062,  2084,  2039,  2005,\n",
       "          1996,  4708,  1010,  2138,  1999,  1037,  2088,  2073,  2498,  6526,\n",
       "          2045,  2003,  2498,  2008,  2064, 15886,  1996, 13972,  1999,  2151,\n",
       "          2126,  1012, 24712,  2038,  2787,  2000,  2224,  1037,  9608,  3815,\n",
       "          1997,  2485,  1011,  2039,  2132,  7171,  2000,  2191,  2009,  2062,\n",
       "          5875,  1998,  2009,  2941,  2573,  3243,  2092,  1012,  2472,  1997,\n",
       "          5855,  1010,  7256,  7369,  1010,  2036,  2038,  1037,  3835,  2126,\n",
       "          1997, 12216,  1996,  4378,  2011,  2007, 23410,  5107,  2592,  1010,\n",
       "          2926,  2012,  2335,  2073,  1037,  2839,  5927,  2242,  1998, 27325,\n",
       "          2000,  2009,  1010,  2021,  2057,  2123,  1005,  1056,  2156,  2009,\n",
       "          2157,   102]])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\n",
    "                   texts_1k[1611] ,\n",
    "                    padding=True,\n",
    "                    truncation=True,\n",
    "                    return_tensors=\"pt\",\n",
    "                    max_length = 512\n",
    "                )['input_ids']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6749db8",
   "metadata": {},
   "source": [
    "# Static vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261db5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, html, unicodedata\n",
    "\n",
    "_WS        = re.compile(r\"\\s+\")\n",
    "_HTML_TAG  = re.compile(r\"<[^>]+>\")\n",
    "_BR        = re.compile(r\"<br\\s*/?>\", flags=re.I)\n",
    "_RATING    = re.compile(r\"\\b\\d{1,2}\\s*/\\s*10\\b\")  # e.g. 8/10, 10/10\n",
    "\n",
    "def minimal_clean(text: str, max_words: int = 800) -> str:\n",
    "    t = unicodedata.normalize(\"NFKC\", str(text))  # unicode normalize\n",
    "    t = html.unescape(t)                          # decode &amp; etc.\n",
    "    t = _BR.sub(\" \", t)                           # <br> -> space\n",
    "    t = _HTML_TAG.sub(\" \", t)                     # drop other tags\n",
    "    t = _RATING.sub(\" \", t)                       # remove 8/10 leaks\n",
    "    t = _WS.sub(\" \", t).strip()  \n",
    "    t=t.lower()                 # collapse spaces\n",
    "   \n",
    "    return t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7dd4898c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class TfidfEmbedder:\n",
    "    def __init__(self,\n",
    "                 analyzer=\"word\",\n",
    "                 ngram_range=(1,2),\n",
    "                 min_df=2,\n",
    "                 max_df=0.9,\n",
    "                 sublinear_tf=True,\n",
    "                 norm=\"l2\",\n",
    "                 preprocessor=minimal_clean):\n",
    "        self.vec = TfidfVectorizer(analyzer=analyzer,\n",
    "                                   ngram_range=ngram_range,\n",
    "                                   min_df=min_df, max_df=max_df,\n",
    "                                   sublinear_tf=sublinear_tf,\n",
    "                                   norm=norm,\n",
    "                                   preprocessor=minimal_clean)\n",
    "    def fit(self, texts):\n",
    "        self.vec.fit(texts)\n",
    "        return self\n",
    "    def transform(self, texts):\n",
    "        return self.vec.transform(texts)\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7a23e255",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "166"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts_1k[17].split())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5ac873",
   "metadata": {},
   "source": [
    "# Training and score computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1aa56f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "correct_A_list = []\n",
    "correct_B_list =  []\n",
    "length_text = []\n",
    "accuracies_A = []\n",
    "accuracies_B = []\n",
    "for fold, (tr,te) in enumerate(skf.split(np.zeros(len(y_1k)), y_1k)):\n",
    "    tr_transformers = All_mini_vectors[tr]\n",
    "    te_transformers = All_mini_vectors[te]\n",
    "    \n",
    "   \n",
    "    y_test = y_1k[te]\n",
    "\n",
    "    tr = list(tr)\n",
    "    te = list(te)\n",
    "\n",
    "    tf_embed = TfidfEmbedder()\n",
    "    tf_vector_train = tf_embed.vec.fit_transform(texts_1k[tr])\n",
    "    tf_vector_test = tf_embed.transform(texts_1k[te])\n",
    "    \n",
    "\n",
    "\n",
    "    model_A = LogisticRegression(max_iter=200).fit(tr_transformers,y_1k[tr])\n",
    "    model_B =  LogisticRegression(max_iter=200).fit(tf_vector_train,y_1k[tr])\n",
    "\n",
    "    y_hat_A =  model_A.predict(te_transformers)\n",
    "    y_hat_B = model_B.predict(tf_vector_test)\n",
    "    \n",
    "    correct_A = (y_hat_A == y_test)\n",
    "    correct_B = (y_hat_B == y_test)\n",
    "\n",
    "    correct_A_list.append(correct_A)\n",
    "    correct_B_list.append(correct_B)\n",
    "    length_text.append([len(texts_1k[i].split()) for i in te])\n",
    "\n",
    "\n",
    "    accuracies_A.append(accuracy_score(y_test,y_hat_A))\n",
    "\n",
    "    accuracies_B.append(accuracy_score(y_test,y_hat_B))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5be366bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies_A = np.array(accuracies_A)\n",
    "accuracies_B = np.array(accuracies_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7d79447a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_value : 0.06109389061093891\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from statistics import mean\n",
    "\n",
    "def paired_perm_folds(acc_A, acc_B, n_perm=10000, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    d = np.asarray(acc_A, float) - np.asarray(acc_B, float)  # paired diffs per fold\n",
    "    delta_obs = d.mean()\n",
    "    k = d.size\n",
    "    diffs = np.empty(n_perm, float)\n",
    "    for i in range(n_perm):\n",
    "        sign = np.where(rng.random(k) < 0.5, 1.0, -1.0)      # flip each pair with p=0.5\n",
    "        diffs[i] = (sign * d).mean()\n",
    "    p = (np.sum(np.abs(diffs)>= abs(delta_obs))+1) / (n_perm+1)  # two-sided\n",
    "    return float(delta_obs), float(p), diffs\n",
    "\n",
    "\n",
    "obs, p, diffs = paired_perm_folds(accuracies_A,accuracies_B)\n",
    "\n",
    "print(f\"p_value : {p}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9b732a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_value : 9.999000099990002e-05\n"
     ]
    }
   ],
   "source": [
    "def paired_perm_instance(correct_A, correct_B, n_perm=10000, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    A = np.asarray(correct_A, int); B = np.asarray(correct_B, int)\n",
    "    delta_obs = A.mean() - B.mean()\n",
    "    N = A.size\n",
    "    diffs = np.empty(n_perm, float)\n",
    "    for i in range(n_perm):\n",
    "        swap = rng.random(N) < 0.5\n",
    "        A_p = np.where(swap, B, A)\n",
    "        B_p = np.where(swap, A, B)\n",
    "        diffs[i] = A_p.mean() - B_p.mean()\n",
    "    p = (np.sum(np.abs(diffs) >= abs(delta_obs)) + 1) / (n_perm + 1)\n",
    "    return float(delta_obs), float(p),diffs\n",
    "obs, p, diffs = paired_perm_folds(correct_A,correct_B)\n",
    "\n",
    "print(f\"p_value : {p}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fd30e575",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PermutationTestResult(statistic=np.float64(0.060699999999999976), pvalue=np.float64(0.0625), null_distribution=array([ 0.0607,  0.0327,  0.0395,  0.0115,  0.0387,  0.0107,  0.0175,\n",
       "       -0.0105,  0.0319,  0.0039,  0.0107, -0.0173,  0.0099, -0.0181,\n",
       "       -0.0113, -0.0393,  0.0393,  0.0113,  0.0181, -0.0099,  0.0173,\n",
       "       -0.0107, -0.0039, -0.0319,  0.0105, -0.0175, -0.0107, -0.0387,\n",
       "       -0.0115, -0.0395, -0.0327, -0.0607]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import permutation_test\n",
    "\n",
    "def statistic(x, y):\n",
    "    return np.mean(y) - np.mean(x)\n",
    "\n",
    "permutation_test((accuracies_A,accuracies_B),statistic,permutation_type='samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "11251bd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(1376)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42cfd928",
   "metadata": {},
   "source": [
    "## Analyze by size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e7605e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_A = np.concatenate(correct_A_list).astype(int)   # shape (N,)\n",
    "correct_B = np.concatenate(correct_B_list).astype(int)   # shape (N,)\n",
    "lengths   = np.array([L for fold_L in length_text for L in fold_L], dtype=int)  # shape (N,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d562ca22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "short  | n=3279 | acc_A=0.836 acc_B=0.885 | Δ=-0.049 | p_perm=0.000\n",
      "medium | n=3380 | acc_A=0.823 acc_B=0.880 | Δ=-0.057 | p_perm=0.000\n",
      "long   | n=3341 | acc_A=0.809 acc_B=0.885 | Δ=-0.076 | p_perm=0.000\n"
     ]
    }
   ],
   "source": [
    "q1, q2 = np.quantile(lengths, [1/3, 2/3])\n",
    "bins = np.digitize(lengths, [q1, q2])   # 0=short, 1=medium, 2=long\n",
    "\n",
    "for b, name in enumerate([\"short\", \"medium\", \"long\"]):\n",
    "    idx = (bins == b)\n",
    "    n = int(idx.sum())\n",
    "    if n < 20:\n",
    "        print(f\"{name}: too few samples (n={n})\"); \n",
    "        continue\n",
    "\n",
    "    acc_A = float(correct_A[idx].mean())\n",
    "    acc_B = float(correct_B[idx].mean())\n",
    "    obs,p, diffs = paired_perm_instance(correct_A[idx], correct_B[idx], n_perm=10000, seed=123)\n",
    "\n",
    "    print(f\"{name:<6} | n={n:4d} | acc_A={acc_A:.3f} acc_B={acc_B:.3f} | Δ={obs:+.3f} | p_perm={p:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadc0dd6",
   "metadata": {},
   "source": [
    "# Vizualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1f596e98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.lines.Line2D at 0x7830aecbe090>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "plt.hist(diffs)\n",
    "plt.axvline(x=obs, color='r', linestyle='--', label='observed difference')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vectorization",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
